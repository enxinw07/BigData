{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"A2B\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "base_path = os.path.abspath('.')\n",
    "\n",
    "checkpoint_directory = os.path.join(base_path, 'checkpoint_directory')\n",
    "os.makedirs(checkpoint_directory, exist_ok=True)\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_version\", StringType(), True),\n",
    "    StructField(\"home_location_lat\", FloatType(), True),\n",
    "    StructField(\"home_location_long\", FloatType(), True),\n",
    "    StructField(\"home_location\", StringType(), True),\n",
    "    StructField(\"home_country\", StringType(), True),\n",
    "    StructField(\"first_join_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "category_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "click_stream_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"booking_id\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", FloatType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", FloatType(), True),\n",
    "    StructField(\"shipment_date_limit\", DateType(), True),\n",
    "    StructField(\"shipment_location_lat\", StringType(), True),\n",
    "    StructField(\"shipment_location_long\", StringType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "path_customer = \"A2A/dataset/customer.csv\"\n",
    "path_category = \"A2A/dataset/category.csv\"\n",
    "path_click_stream = \"A2A/dataset/click_stream.csv\"\n",
    "path_customer_session = \"A2A/dataset/customer_session.csv\"\n",
    "path_product = \"A2A/dataset/product.csv\"\n",
    "path_transaction = \"A2A/dataset/transactions.csv\"\n",
    "\n",
    "df_customer = spark.read.csv(path_customer, schema=customer_schema, header=True)\n",
    "df_category = spark.read.csv(path_category, schema=category_schema, header=True)\n",
    "df_click_stream = spark.read.csv(path_click_stream, schema=click_stream_schema, header=True)\n",
    "df_customer_session = spark.read.csv(path_customer_session, header=True)\n",
    "df_product = spark.read.csv(path_product, schema=product_schema, header=True)\n",
    "df_transaction = spark.read.csv(path_transaction, schema=transaction_schema, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "hostip = \"192.168.0.3\"\n",
    "topic = \"click_stream_topic\"\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"ts\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
    "\n",
    "df = df.select(\n",
    "    col(\"parsed_value.session_id\").alias(\"session_id\"),\n",
    "    col(\"parsed_value.event_name\").alias(\"event_name\"),\n",
    "    col(\"parsed_value.event_id\").alias(\"event_id\"),\n",
    "    col(\"parsed_value.traffic_source\").alias(\"traffic_source\"),\n",
    "    col(\"parsed_value.event_metadata\").alias(\"event_metadata\"),\n",
    "    col(\"parsed_value.customer_id\").alias(\"customer_id\"),\n",
    "    col(\"parsed_value.ts\").cast(TimestampType()).alias(\"event_time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) If the data is late for more than 1 minute, discard it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_watermark = df.withWatermark(\"event_time\", \"60 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  Feature Creation\n",
    "Aggregate the streaming data frame by session id and create features you used in your assignment 2A model. (note: customer ID has already been included in the stream.)   \n",
    "Then, join the static data frames with the streaming data frame as our final data for prediction.  \n",
    "Perform data type/column conversion according to your ML model, and print out the Schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = false)\n",
      " |-- num_cat_lowvalue: long (nullable = false)\n",
      " |-- num_cat_midvalue: long (nullable = false)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- #: string (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- device_type: string (nullable = false)\n",
      " |-- home_location: string (nullable = false)\n",
      " |-- first_join_year: integer (nullable = false)\n",
      " |-- purchase: integer (nullable = false)\n",
      " |-- season: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import month, dayofmonth, datediff, current_date, to_date, col\n",
    "\n",
    "# Define the categories based on event types\n",
    "high_value_actions = ['PURCHASE', 'ADD_PROMO', 'ADD_TO_CART']\n",
    "medium_value_actions = ['VIEW_PROMO', 'VIEW_ITEM', 'SEARCH']\n",
    "low_value_actions = ['SCROLL', 'HOMEPAGE', 'CLICK']\n",
    "\n",
    "# Categorize events based on their type\n",
    "df_categorized = df_click_stream.withColumn(\n",
    "    \"category\",\n",
    "    F.when(F.col(\"event_name\").isin(high_value_actions), \"num_cat_highvalue\")\n",
    "    .when(F.col(\"event_name\").isin(medium_value_actions), \"num_cat_midvalue\")\n",
    "    .otherwise(\"num_cat_lowvalue\")\n",
    ")\n",
    "\n",
    "df_category_counts = df_categorized.groupBy(\"session_id\").pivot(\"category\").count()\n",
    "\n",
    "expected_columns = [\"num_cat_highvalue\", \"num_cat_midvalue\", \"num_cat_lowvalue\"]\n",
    "for column in expected_columns:\n",
    "    if column not in df_category_counts.columns:\n",
    "        df_category_counts = df_category_counts.withColumn(column, F.lit(0))\n",
    "        \n",
    "feature_df = df_category_counts.na.fill({\n",
    "    \"num_cat_highvalue\": 0,\n",
    "    \"num_cat_midvalue\": 0,\n",
    "    \"num_cat_lowvalue\": 0\n",
    "})\n",
    "\n",
    "total_counts = df_category_counts.agg(\n",
    "    F.sum(\"num_cat_highvalue\").alias(\"Total High Value Actions\"),\n",
    "    F.sum(\"num_cat_midvalue\").alias(\"Total Medium Value Actions\"),\n",
    "    F.sum(\"num_cat_lowvalue\").alias(\"Total Low Value Actions\")\n",
    ")\n",
    "\n",
    "df_promotion = df_click_stream.withColumn(\n",
    "    \"is_promotion\",\n",
    "    F.when(F.col(\"event_name\") == \"ADD_PROMO\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_promotion_summary = df_promotion.groupBy(\"session_id\").agg(\n",
    "    F.max(\"is_promotion\").alias(\"is_promotion\")\n",
    ")\n",
    "\n",
    "feature_df = feature_df.join(df_promotion_summary, on=\"session_id\", how=\"left_outer\")\n",
    "\n",
    "feature_df = feature_df.na.fill({\"is_promotion\": 0})\n",
    "\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return \"Spring\"\n",
    "    elif 6 <= month <= 8:\n",
    "        return \"Summer\"\n",
    "    elif 9 <= month <= 11:\n",
    "        return \"Autumn\"\n",
    "    elif month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "\n",
    "season_udf = F.udf(get_season, StringType())\n",
    "\n",
    "df_click_stream = df_click_stream.withColumn(\n",
    "    \"month\", F.month(\"event_time\")\n",
    ").withColumn(\n",
    "    \"season\", season_udf(\"month\")\n",
    ")\n",
    "\n",
    "df_season = df_click_stream.groupBy(\"session_id\").agg(\n",
    "    F.max(\"season\").alias(\"season\")  \n",
    ")\n",
    "\n",
    "df_customer = df_customer.withColumn(\"first_join_date\", F.to_date(F.col(\"first_join_date\")))\n",
    "df_customer = df_customer.withColumn(\"first_join_year\", F.year(F.col(\"first_join_date\")).cast(\"integer\"))\n",
    "\n",
    "current_year = datetime.now().year\n",
    "current_date = datetime.now()\n",
    "df_customer = df_customer.withColumn(\"birthdate\", F.to_date(col(\"birthdate\"), \"dd-MM-yyyy\"))\n",
    "df_customer = df_customer.withColumn(\"age\", (F.datediff(F.current_date(), col(\"birthdate\")) / 365.25).cast(\"integer\"))\n",
    "\n",
    "df_customer_after = df_customer.select(\n",
    "    \"customer_id\", \"gender\", \"age\", \"device_type\", \"home_location\", \"first_join_year\"\n",
    ")\n",
    "\n",
    "df_customer_after_01 = df_customer_session.join(df_customer_after, \"customer_id\")\n",
    "feature_df = feature_df.join(df_customer_after_01, \"session_id\")\n",
    "\n",
    "df_transaction = df_transaction.withColumn(\"purchase\", F.when(F.col(\"payment_status\") == \"Success\", 1).otherwise(0))\n",
    "\n",
    "df_transaction = df_transaction.select(\"session_id\", \"customer_id\", \"purchase\")\n",
    "\n",
    "feature_df = feature_df.join(df_transaction, on=[\"session_id\", \"customer_id\"], how=\"left\")\n",
    "\n",
    "feature_df = feature_df.na.fill({\"purchase\": 0})\n",
    "\n",
    "feature_df = feature_df.join(df_season,\"session_id\")\n",
    "\n",
    "required_columns = [\n",
    "    \"session_id\", \"customer_id\",\n",
    "    \"num_cat_highvalue\", \"num_cat_midvalue\", \"num_cat_lowvalue\",\n",
    "    \"is_promotion\",\n",
    "    \"season\", \"gender\", \"age\", \"device_type\", \"home_location\", \"first_join_year\",\n",
    "    \"purchase\"  \n",
    "]\n",
    "\n",
    "for column in required_columns:\n",
    "    if column not in feature_df.columns:\n",
    "        feature_df = feature_df.withColumn(column, F.lit(None))\n",
    "\n",
    "default_values = {\n",
    "    \"gender\": \"Unknown\", \"age\": -1, \"device_type\": \"Unknown\",\n",
    "    \"home_location\": \"Unknown\", \"first_join_year\": -1\n",
    "}\n",
    "feature_df = feature_df.na.fill(default_values)\n",
    "\n",
    "df_final = feature_df.join(df_with_watermark, [\"session_id\", \"customer_id\"])\n",
    "\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load your ML model, and use the model to predict if a purchase is made or not in each session. Persist the prediction result in parquet format, then read the parquet result and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "model_path = \"A2A/Moth_model\" \n",
    "model = PipelineModel.load(model_path)\n",
    "\n",
    "predictions = model.transform(df_final)\n",
    "\n",
    "predictions_1 = predictions.select(\"rawPrediction\", \"probability\", \"prediction\")\n",
    "\n",
    "output_path = \"checkpoint/predictions_output\"\n",
    "query = predictions_1.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+\n",
      "|rawPrediction|probability|prediction|\n",
      "+-------------+-----------+----------+\n",
      "+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.read.parquet(output_path)\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Using the prediction results, write code to process the data following the requirements below and show results.\n",
    "a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sales = predictions.withColumn(\"event_time\", F.col(\"event_time\").cast(\"timestamp\")) \\\n",
    "    .filter(predictions.prediction == 1) \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(F.window(predictions.event_time, \"1 minute\", \"10 seconds\")) \\\n",
    "    .agg(F.count(\"prediction\").alias(\"total_potential_sales\")) \\\n",
    "    .select(\"window.start\", \"window.end\", \"total_potential_sales\")\n",
    "\n",
    "query_sales = potential_sales.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Every 30 seconds, show the total potential revenue in the last 30 seconds. “Potential revenue” here is defined as When prediction=1, extract customer shopping cart detail from metadata (sum of all items of ADD_TO_CART events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "import json\n",
    "\n",
    "def extract_cart_total(metadata):\n",
    "    try:\n",
    "        data = json.loads(metadata)\n",
    "        items = data.get(\"items\", [])\n",
    "        return sum(item.get(\"price\", 0) for item in items)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "extract_cart_total_udf = F.udf(extract_cart_total, DoubleType())\n",
    "\n",
    "potential_revenue = predictions.filter(predictions.prediction == 1) \\\n",
    "    .withColumn(\"cart_total\", extract_cart_total_udf(predictions.event_metadata)) \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(F.window(predictions.event_time, \"30 seconds\", \"30 seconds\")) \\\n",
    "    .agg(F.sum(\"cart_total\").alias(\"total_potential_revenue\")) \\\n",
    "    .select(\"window.start\", \"window.end\", \"total_potential_revenue\")\n",
    "\n",
    "query_revenue = potential_revenue.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Every 1 minute, show the top 10 best-selling products by total quantity. (note: No historical data is required, only the top 10 in each 1-minute window.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_item_quantities(metadata):\n",
    "    try:\n",
    "        data = json.loads(metadata)\n",
    "        items = data.get(\"items\", [])\n",
    "        return [(item.get(\"id\", \"unknown\"), item.get(\"quantity\", 0)) for item in items]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "extract_item_quantities_udf = F.udf(extract_item_quantities, ArrayType(StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])))\n",
    "\n",
    "exploded_items = predictions.filter(predictions.prediction == 1) \\\n",
    "    .withColumn(\"items\", F.explode(extract_item_quantities_udf(predictions.event_metadata))) \\\n",
    "    .select(\"event_time\", \"items.id\", \"items.quantity\") \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\")\n",
    "\n",
    "top_selling_products = exploded_items.groupBy(F.window(exploded_items.event_time, \"1 minute\"), \"id\") \\\n",
    "    .agg(F.sum(\"quantity\").alias(\"total_quantity\")) \\\n",
    "    .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"window\").orderBy(F.desc(\"total_quantity\")))) \\\n",
    "    .filter(\"rank <= 10\") \\\n",
    "    .select(\"window.start\", \"window.end\", \"id\", \"total_quantity\")\n",
    "\n",
    "query_top_products = top_selling_products.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
